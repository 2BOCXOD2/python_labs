
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True):
    if type(text) == str:
        if casefold == True: # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –¥–µ–ª–∞–µ—Ç –≤—Å–µ –±—É–∫–≤—ã —Å—Ç—Ä–æ—á–Ω—ã–º–∏
            text = text.casefold()
        if yo2e == True: # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –¥–µ–ª–∞–µ—Ç –≤—Å–µ –Å/—ë –±—É–∫–≤–∞–º–∏ –ï/–µ
            text = text.replace("—ë", "–µ")
            text = text.replace("–Å", "–ï")
        text = text.split() # –£–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        new_text = ""
        lenn = len(text)
        for i in range(lenn): # –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–∞–¥ –≤ —Å—Ç—Ä–æ–∫—É
            if i < lenn - 1:
                new_text = f"{new_text + text[i]} "
            else:
                new_text = f"{new_text + text[i]}"
        new_text = new_text.replace("\n", '') # –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        new_text = new_text.replace("\r", '')
        new_text = new_text.replace("\t", '')
        return new_text
    else:
        raise TypeError

# textt = "   \n–ï–µ–Å—ë   \r–ê–ë–≤–≥–î   \t12(%‚Ññ?*)?;         "
# print(normalize(textt, casefold=True, yo2e=True))

def tokenize(text: str):
    if type(text) == str:
        alf = ",.!_;:?üòÄ" # –°–∏–º–≤–æ–ª—ã-–ø—Ä–æ–±–µ–ª—ã
        text = text.split()
        lenn = len(text)
        new_text = ''
        for i in range(lenn): # –£–¥–∞–ª–∏–ª–∏ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ç–∏–ø —Å—Ç—Ä–æ–∫–∏
            if i < lenn - 1:
                new_text = f"{new_text + text[i]} "
            else:
                new_text = f"{new_text + text[i]}"
        for a in alf: # –£–¥–∞–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã-–ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ç–∏–ø —Å—Ç—Ä–æ–∫–∏
            text = new_text
            text = text.split(a)
            lenn = len(text)
            new_text = ""
            for j in range(lenn):
                if j < lenn - 1:
                    new_text = f"{new_text + text[j]} "
                else:
                    new_text = f"{new_text + text[j]}"
        new_text = new_text.split()
        return new_text
    else:
        raise TypeError
# texxt = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"
# print(tokenize(texxt))

def count_freq(dannye):
    new_dannye = []
    for i in dannye:
        if i in new_dannye:
            pass
        else:
            new_dannye.append(i)
    slovar = {}
    for z in new_dannye:
        col = dannye.count(z) # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π
        slovar[z] = col
    return slovar
def top_n(dictt, n_top):
    a = []
    for key, value in dictt.items():
        a.append((key, value))
    otv = a.sort() # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    otv = sorted(a, key = lambda x: (x[1]), reverse=True) # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    otv_1 = []
    for i in range(n_top): # –í—ã–≤–æ–¥ —Ç–æ–ø–∞
        otv_1.append(otv[i])
    return otv_1
# spisok = ["bb","aa","bb","aa","cc"]
# n = 2
# print(count_freq(spisok))
# print(top_n(count_freq(spisok), n))
